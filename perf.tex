\todo{Brief paragraph summarizing what are we going to study
Description of the experimental platform HW OS Compilers,
OpenBLAS
} 

\begin{figure*}[]
\includegraphics[width=\linewidth]{figures/mnist-rel-abs-time.pdf}
\caption{\todo{MNIST - Relative and absolute execution layer time.}}
\end{figure*}

\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/mnist-scalability-layer.pdf}
\caption{MNIST - Layer Scalability}
\end{figure*}

\subsection{MNIST dataset}
For the performance analysis of the MNIST dataset we have first 
developped a per-layer study bothg coarse-grain and fine-grain parallelizations.For the coarse-grain case we identify what are the main limiting 
peformance factors. Then we describe the overall performance of the 
coarse-grain parallelization. 

\subsubsection{Coarse-grain Layer Performance}
Figure \ref{fig-mnist-abs-rel} shows the absolute execution time per layer 
and the relative weight in the overall execution time. Horizontal bars 
correspond to executions with 1, 2, 4, 8, 12 and 16 threads. In general, 
two layers dominate the whole execution: the convolutional and pooling 
layers. No matter the number of threads, these two type of layers always 
account for almost 80\% of total execution time, adding their forward 
and backward phases. Notice that there are different instances of the 
same type of layer but with very different absolute execution time. 
For instance, the \emph{conv1} and conv2 layers, and in a smaller magnitude 
the pool1 and pool2 layers. After the convolutional and pooling layers, 
the next significant layer is the inner product ip1. The rest of layers, 
expose a very small contribution to the overall execution time 
(e.g.: loss, relu and ip2 layers in their forward and backward phases).
In general, notice that in each horizontal bar there is a zone where the 
work in each layer phase decreases. This corresponds to the center part, 
composed of the forward and backward phases of pool2, ip1, relu, ip2 and loss.
This behaviour is associated to the dimensionality reduction that neural 
networks do, and affects the work granularity for the parallelization process.
Moreover, this behaviour limits the overall scalability of the training process.

Figure \ref{fig-mnist-scalability} shows the scalability curve of each layer. 
We identify three layer behaviours. First, notice the u-shape of the 
scalability trends for any number of threads. The center points 
correspond to the layers that we have previously identified as not 
significant in the overall execution time (e.g.: loss, relu and ip2 layers). 
These layers do not scale at all, but they do not represent a limitation for 
the overall performance. The two sides on the center values correspond 
to the forward and backward phases of the rest layers. For these, we detect 
two types of layer behaviour. 

Layers ip1 and pool2 present very poor scalability curves. 
For ip1, in both the forward and backward phases, the layer 
presents speedups of 4,58 and 5,93 for the forward and backward 
phases respectively and with 8 threads. The layer does not improve 
the speedup with more threads. The pool2 layer exposes the same 
behaviour with maximum speedup of 5.52 and 5.73 with 8 threads in 
the forward and backward phases respectively. The reason for this 
behavior is two fold. First, notice the two layers are immediately 
stacked one on top of the other. This means that the 
output of the pool2 layer is the input for the ip1 layer. It happens 
that the blob shapes between the two layers do not match. The pool2 
layer is parallelized according to its input blob dimensions, and 
produces its output blob (input blob for ip1) following the resulting 
work and data distribution coming out from the parallelization. When 
it comes the execution of the ip1 layer, its parallelization is done 
according to its input blob dimensions (output blob of pool2), which do 
not match those of the input blob for the pool2 layer. Thus, theres is 
an unvoidable lost of locality for the execution of the ip1 layer. 
Second, both layers suffer from a poor granularity when executing with 
more thatn 8 threads. In Figure \ref{fig-mnist-abs-rel} we observe that 
with more than 8 threads the forward and backward phases of the two layers 
are in the range of 350 microseconds.

Layers conv1, pool1 and conv2 present good scalability curves. They 
correspond to the layers in both sides of the center part of the 
scalability layer curves. In general, these layers respond well to the 
increments on the number of threads. This is explained by two factors. 
First, all these layers expose a considerable amount of work, as it has 
been indicated with Figure \ref{fig-mnist-abs-rel}. Second, all of them 
are stacked one next to each other within the network, and all of them 
match their input/output blob dimensions. Thus, data locality is 
preserved along the their execution in both the forward and backward phases.
We have detected that although being exactly the same layer, the conv1 
and conv2 layers have close to a 10\% difference in speedup. 
In particular, the conv2 layer exposes greater speedups than conv1. 
This happens more noticeably with more than 8 threads and for the 
forward phase. The difference between the two layers is the position 
they occupy in the layer stack. Conv1 is the immediate layer after the 
data layer in the MNIST network. The data layer is responsible for
feeding the network with data samples. The layer executes sequentially, 
so the data associated to all images generates a memory footprint that is 
not matching the one generated along the parallel execution of the 
conv1 layer. Therefore, the conv1 layer suffers from a poor data 
locality with its immediate previous layer.

%\begin{figure}[]
%\includegraphics[width=\linewidth]{figures/mnist-gpu-layer-speedup.pdf}
%\caption{\todo{CAPTION}}
%\end{figure}

\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/mnist-abs-perf+gpu-layer.pdf}
\caption{MNIST - ABS PERF + GPU LAYER }
\end{figure*}

\subsubsection{Fine-grain Layer Performance}
The fine-grain layer parallelization is available in Caffe in two versions. 
All available layers come with a native GPU implementation of their 
forward and backward phases. We identify this version as the \emph{plain-GPU} 
version. Specifically for the convolutional and pooling layers, Caffe 
includes a cuDNN-based version. We identify this version as the 
\emph{cuDNN-GPU} version. 
%Both present similar execution time distribution 
%as the omp-1 serie in Figure \ref{fig-mnist-abs-rel}: convolutional and 
%pooling layers dominate the GPU execution. 
Rightmost side of Figure \ref{fig-mnist-abs} shows the per-layer 
speedup numbers for the plain-GPU and cuDNN-GPU versions. 
For the plain-GPU version, all layers present speedupd below 
the 10x bar unless for specific exceptions. The pool1 and pool2 
layers expose extraordinary speedups of 57x and 62x for their 
forward phase respectively. The pool2 layer presents a speedup of 
12.81x in its backward phase and the ip1 layer is also above the 
10x bar with a speedup of 12.25x in its backward phase.
In contrast, the convolutional layers present a very poor speedup, with 
1.11, 1.63 for ther forward phases of conv1 and conv2, and 0.43x and 2.86x 
in their respective backward phases. 

For the cuDNN-GPU, the results are similar, unless for the convolutional 
and pooling layers. The conv1 and conv2 layers experiment an extraordinary 
improvement reaching speedups of 15x, 25x, 19x and 8x in their 
forward and backward phases. In contrast, the pool2 layer experiments a 
dramatic loss of performance: it drops from 62x to 27x in its forward 
phase and from 12.81x to 8.81x in its backward phase. More moderately, 
the relu layer also suffers a performance drop from 2.47x to 1.74x and 
from 4x to 2,41x in its froward and backward phases respectively. 
In general, cuDNN corresponds to a case where the industry has deployed 
a highly optimized implementation of layer transformations that well 
understood and no longer in a research stage. In this situation, the 
fine-grain parallelism makes a difference, after the corresponding recoding 
efforts.

%\begin{figure}[b]
%\includegraphics[width=\linewidth]{figures/mnist-abs-perf-all.pdf}
%\caption{\todo{CAPTION}}
%\end{figure}

\subsubsection{Overall Performance}
Figure \ref{fig-mnist-overall} shows the overall performance of
the coarse-grain parallelization and the fine-grain parallelization in 
its two versions GPU and GPU-cuDNN. The coarse-grain reaches a speedup 
close to a 6x with 8 threads, and 8x with 16 threads. The lack of 
the scalability for the CPU version is related to the poor scalability 
of fine-grained layers that when executing with 16 threads drag down 
the performance. In addition, we suspect the serial initialization of 
the network structures is giving a suboptimal memory allocation in 
the NUMA nodes. All of this is affecting the final scalability of 
the coarse-grain version. The fine-grain GPU version shows a 
modest speed up close to 2x. The reason for this difference is related 
to the performance of the convolutional layers. In general, this version 
corresponds to a base line defined by the Caffe native implementation 
of the GPU acceleration. It represents a case for the performance the DNN 
community can obtain with a fine-grain parallelization and very significant 
coding efforts. Remember that within Caffe, all layers have to have 
both a CPU and GPU implementation to guarantee GPU acceleration. 
In conclusion, the coarse-grain approach minimizes the coding efforts 
and delivers better performance levels. Of course, when compared to 
the cuDNN case, the fine-grain approach makes a difference. 
It delivers a 12x speedup. But solutions like the cuDNN framework are 
only available when the layer types and their implementation have become 
a product and are no longer in a research stage. Thus, they can have a 
highly optimized implementation. For a DNN framework like Caffe, 
which is aimed to give support for research in new network architectures 
with novel layer types, the fine-grain parallelism imposes hard recoding 
efforts. In contrast, the coarse-grain option is much more immediate and 
effective.

\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/cifar-abs-rel-time.pdf}
\caption{\todo{CIFAR - Relative and absolute execution layer time.}}
\end{figure*}

\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/cifar-scalability-layer.pdf}
\caption{\todo{CIFAR - Layer scalability.}}
\end{figure*}

\subsection{The CIFAR-10 case}
For the performance analysis of the CIFAr dataset we have followed the 
same methodology as for the MNIST case. First, we have developped a 
per-layer study both coarse-grain and fine-grain parallelizations. 
For the coarse-grain case we identify what are the main limiting 
peformance factors. Then we describe the overall performance of the 
coarse-grain parallelization. 

\subsubsection{Coarse-grain Layer Performance}
The CIFAr-10 dataset generates a work granularity greater than 
the MNIST case. This can be observed in Figure \ref{fig-cifar-abs-rel}. 
The figure shows per-layer absolute execution time and the relative 
weigth in the overall execution time. It is clear that just a few 
layers dominate the execution, no matter the number of threads. 
These layers are the convolutional and pooling layers, and with 
less weight the local response normalization layers. In general, 
these layers account for almost 85\% of total execution time in 
all thread configurations. Therefore, the layers with very small 
granularity and exposing very poor scalability curves will not 
determine the overall performance. Only the scalability of these 
dominating layers will determine the effectiveness of the 
parallelization.

Figure \ref{fig-cifar-scalabitly} shows the scalability curves of all layers. 
Notice the appearance of the u-shape form as long as the number of threads 
increases. The center part of each cluster corresponds to layers of very 
small granularoty which do not affect the overall performance. These layers 
are the pool3, ip1 and loss. The center part is composed of the forward 
and backward phases of these layers.

The CIFAR-10 network is organanized in three levels all of them with a 
similar organization. First level corresponds to a sequence of a data 
layer plus conv1+pool1+relu1+norm1. The data layer fetches the input 
data (e.g: the batch images) sequentially so the conv1 layer suffers from 
poor lacality respect its input data (the same situation observed with 
the MNIST case). The work distribution is constant across the conv1, 
pool1 and relu1 layers, and changes for the norm1. This implies  

\subsubsection{Fine-grain Layer Performance}
\todo{Similar as MNIST}

\subsubsection{Overall Performance}
\todo{Similar as MNIST}

\subsection{Coarse-grain Parallelization: Pros and Cons}
We have identified several factors that affect the batch-level
parallelization of the Caffe DNN framework. These factors are
layer dependant and are mainly related to specificities of the
layers that determine their work distribution, memory footprint,
data privatization and ordered operations. The following subsections 
describe these limiting factors.

\textbf{Network agnostic}:

\textbf{Convergence invariance }:

\textbf{Sequential memory allocation}: the network memory allocation
happens during the network initialization. This process is sequential, 
which causes the layer memory be allocated following a
pattern generated by the initialization code. In terms of performance, 
this pattern is not compatible with those that arise during
the training process.

\textbf{Locality between layers:} the input/output relation across layers
defines a lost of data locality for specific layers. During the forward 
and backward phases, each layer distributes the work according to the 
dimensions of the input blobs. Regarding the data locality, it is 
possible that input and output blobs do not match
their dimensions. Consequently, the work distribution and thread-
data association defined in one layer will not match that one of the
next immediate layer in the stack, causing some data movement
across the memory hierarchy.

One particular case of this situation corresponds to the data
layers in Caffe. These layers feed the network with input data
organized in batches. Data layers execute in sequentially.
Therefore, a one thread first accesses all data and then the data is
distributed across the cores and the memory hierarchy when the
first parallel processing layer in the network is executes in parallel. 

\textbf{Work unbalance:} coarse grain parallelism is open to work unbalance. 
For the Caffe case, we have detected that batch-level parallelism 
defines very heavy iterations for the parallelized loops.
Therefore, one single loop iteration can cause a high unbalance
between the executing threads. In general, threads receive the
same number of iterations (e.g.: under a static scheduling). But it
is possible that some of the threads receive one more iteration
than other threads when the number of samples in the batch is not

\textbf{Work granularity:} as previously described, neural networks try
to perform a dimensionality reduction over the processed data.
And this affects the size of the working sets in the network layers.
At some level in the network, the input/output blobs start decreasing their sizes. When this happens, a thread level parallelization
starts suffering from too small work granularity with poor performance levels.

\textbf{Data privatization and reduction operations:} specifically in the
backward phase, the network coefficients are updated per each
sample in the batch. At batch-level, this requires mutual exclusion
mechanisms to guarantee a correct update. Data privatization has
been needed for this purpose. Caffe is implemented following an
object-oriented design and object privatization (e.g.: blobs) has
performance implications regarding the invocation of object constructor/destructor.


