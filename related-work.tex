\hyphenation{DistBelief}
This section describes some state of the art deep learning frameworks.
In \cite{JeffDean2012} the authors present a software framework called \emph{DistBelief} that enables model parallelism within a machine (via multithreading) and across machines (via message passing), with the details of parallelism, synchronization and communication managed by
the framework. In addition to supporting model parallelism, the \emph{DistBelief} framework also supports
data parallelism, where multiple replicas of a model are used to optimize a single objective function.

To facilitate training of very large deep networks \emph{DistBelief} supports distributed computation in neural networks and layered graphical models. The user defines the computation that takes place at each node in each layer of the model, and the messages that should be passed during the upward and downward phases of computation.The authors mention in the paper that in certain cases partitioning the model on more than certain number of machines actually slows down training, as communication overhead across hosts starts to dominate in fully-connected layers. One of the main disadvantages of the this framework is that one has to efficiently map the compute blocks onto the host processors such that communication across hosts are minimized to maximally leverage the model parallelism. The coarse level parallelism proposed in this paper is \textbf{network-agnostic} and hence more universally applicable.

In \cite{Coates2013}  presents a deep learning framework with COTS HPC systems: a cluster of GPU servers with Infiniband interconnects and MPI. Their system was able to train 1 billion parameter networks on just 3 machines in a couple of days, and they showed that it can scale to networks with over
11 billion parameters using just 16 machines. While the \emph{DistBelief} framework manages to train a neural network using 16000
CPU cores (in 1000 machines) in just a few days, yet this level of resource is likely well beyond those available to most deep learning researchers.
In this paper the authors present an alternative approach for training large scale networks that leverages inexpensive
computing power in the form of GPUs and introduces the use of high-speed communications infrastructure to tightly coordinate distributed gradient
computations. The authors in this paper makes significant efforts to make the most efficient use of the HPC resources, which may be sometimes well beyond the capabilities of machine learning researchers.

In \cite{Hannun2014} presents a state-of-the-art speech recognition system developed using deep learning.
Their approach uses a well-optimized RNN (recurrent neural network) training system that uses multiple GPUs and use of a novel model partition
scheme to improve parallelization. The authors in this paper develop specific data layout techniques and optimization schemes to scale their training for recurrent networks. The cuDNN \cite{chetlur2014cudnn} libraries are not general enough and doesn't provide an API for efficiently mapping recurrent computations on the GPU. The coarse grained parallelism proposed in this paper is a simplistic approach that can inherently applied for any feed-forward or recurrent neural network. Hence the proposed parallelism is more universally applicable across diverse application domains.

