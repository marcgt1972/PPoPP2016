This section describes the parallelization process for a coarse-grain 
parallelization in the Caffe DNN framework. First, we identify the 
different sources of parallelism and their granularity. Second, we 
describe the methodology to generate a coarse-grain parallelism version 
of the algorithms discussed previously in section \ref{sec-caffe}. Finally, 
we compare our approach to that of a fine-grain parallelization 
corresponding to the GPU version.

\subsection{Sources of Parallelism}
Given the structure of the computation in algorithms \ref{alg-train}, 
\ref{alg-forw} and \ref{alg-back}, the training procedure of a neural 
network exposes several levels and types of parallelism.

\subsubsection{BLAS Level Parallelism}
This level of parallelism corresponds to the computations that 
are based on basic linear algebra operations. It appears in the 
BLAS computations executed for each data segment in the input/output blobs.
In algorithms \ref{alg-forw} and \ref{alg-back} the BLAS calls in line 
6 in both cases hide this level of parallelism.
In general, these computations correspond to matrix and
vector operations like matrix-$\times$-vector and matrix-$\times$-matrix
products. Caffe includes a native and limited BLAS implementation that 
can be substituted with specialized libraries like Atlas, OpenBLAS  or Intel MKL \cite{blackford2002updated,dongarra2002preface}. These implementations are highly optimized and exploit both thread level parallelism and SIMD level parallelism with vectorized code.

\subsubsection{Blob Level Parallelism}
This level of parallelism corresponds to the nested loops in the forward
and backward pass in lines 2 and 5 in algorithms 
\ref{alg-forw} and \ref{alg-back}. 
These loops traverse the data segments in the input and output blobs and
perform BLAS calls for each segment. This kind of parallelism can be
achieved with thread level parallelism as each BLAS call can be
executed in parallel.
In general, the dimensions of the input blob are layer dependent. Therefore the number of iterations in each loop level is also layer dependent. All loops are 
interchangeable and parallel, given the appropriate data privatizations. 
So, in order to have control of the most appropriate loop scheduling, 
loops can be rearranged in different manners. 

\subsubsection{Batch Level Parallelism}
This level corresponds to the loop in line 1 of algorithms \ref{alg-forw}
and \ref{alg-back}. 
This loop traverses the data samples in the batch and its parallelism 
can be exploited but with limitations. 
Specifically for the backward pass, it requires reduction operations 
for the network coefficient update. The training algorithm averages all 
gradients computed with each sample in the batch. This averaging
has to be protected with mutual exclusion operations or ordered
loops plus data privatization for the blobs that temporally store the
gradients. 


%\begin{algorithm}
%\caption{Caffe layer forward phase}
%\label{batch-forw-algo}
%%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%\Input{Bottom blob (N, $D_1, D_2$, $\dots$, $D_N$)}
%\Output{Top blob for the layer transformation}
%\BlankLine
%\For{$s \leftarrow 1$ \KwTo $S$}{
%\For{$d_1 \leftarrow 1$ \KwTo $D_1$}{
  %\For{$d_2 \leftarrow 1$ \KwTo $D_2$}{
%\BlankLine
    %\dots
%\BlankLine
    %\For{$d_N \leftarrow 1$ \KwTo $D_N$}{
      %top[f($d_1,d_2$, $\dots$, $d_N$)]=\textbf{BLAS}(W, bias, bottom[g($d_1,d_2$, $\dots$, $d_N$)]);
%}
%}
%}
%}
%\end{algorithm}

\begin{algorithm}
\setstretch{0.25}
\small
%\algsetup{linenosize=\footnotesize}
\caption{Coarse-grain parallel layer forward pass}
\label{alg-par-forw}
%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Bottom blob (S, N+1, $D_1, D_2$, $\dots$, $D_N$)}
\Output{Top blob for the layer transformation}
%\BlankLine
%\#pragma omp parallel for 
\textbf{\#pragma omp parallel}
%\BlankLine
\{
\BlankLine
\emph{/* Object Privatization */}
%\BlankLine
\dots
\BlankLine
\textbf{\#pragma omp for private($s$, $d_1$, $d_2$, \dots)}
\BlankLine
\For{$civ \leftarrow 1$ \KwTo $S*D_1$$*D_2$*$\dots$*$D_k$}{
    %\BlankLine
    s = $f_s$(civ);
    \BlankLine
    $d_1$ = $f_1$(civ);
    \BlankLine
    $d_2$ = $f_2$(civ);
    \BlankLine
    \dots
    \BlankLine
    $d_k$ = $f_k$(civ);
    \BlankLine
    \For{$d_{\text{k+1}}$ $\leftarrow 1$ \KwTo $D_{\text{k+1}}$}{
    \BlankLine
    \dots
    \BlankLine
    \For{$d_N \leftarrow 1$ \KwTo $D_N$}{
      top[f(s, $d_1,d_2$, $\dots$, $d_N$)]=\textbf{BLAS}($W$($d_1,d_2$, $\dots$, $d_N$), bias($d_1,d_2$, $\dots$, $d_N$), bottom[g(s, $d_1,d_2$, $\dots$, $d_N$)]);
}
}
}
\}
\end{algorithm}

\subsection{Code Transformations}
The Caffe framework is designed using object-oriented methodologies and uses a C++ implementation. 
Its main data structures and algorithms have been designed with 
functional principles with the aim of giving support for neural network 
development. In this paper, we have followed a methodology to 
parallelize Caffe so that these principles are kept intact. We wanted to minimize 
code reorganization for the optimization process.
To indicate the parallelism to be exploited, only directive-based 
transformations have been applied. We have used OpenMP \cite{basumallik2007programming} directives 
to indicate which loops have to be parallelized. Whenever possible, 
we have used the OpenMP primitives to indicate data privatizations 
and reduction operations, as well as the necessary synchronization 
points along the parallel code. To increase performance, we have 
applied very simple manual code transformations like loop coalescing or 
loop interchange. Therefore, in order to enable a coarse-grain 
parallelization, no object data structure has been modified and 
the implementation of every Caffe layer has not been recoded with 
any platform-specific optimization.

\subsubsection{Coarse grain parallelization and optimizations}
Algorithm \ref{alg-par-forw} shows a high level description of a 
coarse grain parallelization of the forward layer transformation 
described in section \ref{sec-caffe}. The parallelization is specified 
with directive-based annotations following the OpenMP syntax and semantics. 
Coarse parallelism is defined by a parallel region that englobes all the 
layer code (lines 1-16). The original loop nest appears with a coalescing 
transformation where some of the outermost loops have been collapsed into a 
single loop statement (line 6). A parallelizing directive for this loop 
is inserted (line 5) specifying the parallel execution of this loop 
under a static loop scheduling (default scheduling for OpenMP \cite{basumallik2007programming}). 
The loop coalescing transformation is related to the parallelization 
process and is done to have effective control of the work distribution 
under a static loop scheduling. To understand this issue, recall the 
code structure in Algorithm \ref{alg-forw}. In that version, the outermost 
loop corresponds to the loop that processes every data sample in the 
batch (loop with variable \emph{s}). In general, after the 
parallelization under a static scheduling, one iteration is the minimal 
work unit for work distribution. If that loop were parallelized, the 
amount of work per iteration is coarse enough so that any difference 
in the number of assigned iterations per thread can cause a significant  
work unbalance. To maintain the coarse level parallelization, but minimize 
the size of the work unit, the coalescing transformation increments the 
total number of iterations, but having every iteration a smaller 
amount of work. In general, the number of coalesced loops is layer 
dependent. Some of the layers are parallelized with no coalescing, 
other coalesce the whole loop nest. This optimization process was 
manually performed.

Algorithm \ref{alg-par-back} shows the coarse-grain parallelization of 
the backward layer pass. For parallelism specification and work 
distribution, the transformations are the same as in the previous 
case for the layer forward pass. The same coalescing transformation has 
been applied and parallelism has been specified at the same levels. 
The number of coalesced loops is layer dependent, and this optimization 
has been manually performed. The main difference resides on the special 
treatment for the gradient update (\emph{diffs\_top} and \emph{diffs\_bottom} 
variables). Recall that the DNN training algorithm averages all gradients 
computed within a batch. This happens during the layer backward pass.
Now, this computation has been parallelized, so the gradient update has 
to be implemented through a reduction operation and thread mutual exclusion 
mechanisms. In the algorithm, this corresponds to lines 18-20. An ordered loop 
is added so that every thread incorporates its gradient computation to 
the global variable that stores the gradients.
This mechanism requires the per-thread privatization of the blob storing 
the gradients. The private storage for the gradients has to be properly 
initialized to the neutral value of the reduction operation, in the case 
the zero value (lines 4-5 in the algorithm). Notice that in order to 
ensure convergence invariancy, the ordered construct is necessary. 
A critical section solution would not ensure an update following the 
original sequential order.

\begin{algorithm}
\setstretch{0.15}
\small
\caption{Coarse-grain parallel layer backward pass}
\label{alg-par-back}
%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Gradient w.r.t. top blob (S, N+1, $D_1, D_2$, $\dots$, $D_N$)}
\Output{Gradient w.r.t. to bottom blob (M, $O_1, O_2$, $\dots$, $O_M$)}
\BlankLine
%\#pragma omp parallel for 
\textbf{\#pragma omp parallel}
\BlankLine
\{
\BlankLine
\emph{/* Object Privatization */}
\BlankLine
Blob private-diffs\_bottom(M, $O_1, O_2$, $\dots$, $O_M$);
\BlankLine
caffe\_zero($S*O_1$$*O_2$*$\dots$*$O_M$, private-diffs\_bottom);
\BlankLine
\dots
\BlankLine
\textbf{\#pragma omp for private($s$, $d_1$, $d_2$, \dots)}
\BlankLine
\For{$civ \leftarrow 1$ \KwTo $S*D_1$$*D_2$*$\dots$*$D_k$}{
    \BlankLine
    s = $f_s$(civ);
    \BlankLine
    $d_1$ = $f_1$(civ);
    \BlankLine
    $d_2$ = $f_2$(civ);
    \BlankLine
    \dots
    \BlankLine
    $d_k$ = $f_k$(civ);
    \BlankLine
    \For{$d_{\text{k+1}}$ $\leftarrow 1$ \KwTo $D_{\text{k+1}}$}{
    \BlankLine
    \dots
    \BlankLine
    \For{$d_N \leftarrow 1$ \KwTo $D_N$}{
      private-diffs\_bottom[f(s, $d_1,d_2$, $\dots$, $d_N$)]=\textbf{BLAS}($W$($d_1,d_2$, $\dots$, $d_N$), bias($d_1,d_2$, $\dots$, $d_N$), top[h(s, $d_1,d_2$, $\dots$, $d_N$)], diffs\_top[g(s, $d_1,d_2$, $\dots$, $d_N$)]);
}
}
}
\textbf{\#pragma omp for ordered}
\BlankLine
\For{$th \leftarrow 1$ \KwTo $omp\_get\_num\_threads()$}{
  caffe\_add(diffs\_bottom, private-diffs\_bottom, \dots);
}
\}
\end{algorithm}
\subsection{Coarse-grain vs Fine-grain parallelization}
The aim of this section is to describe the main differences between 
the coarse-grain and fine-grain approaches when it comes to the 
parallelization of a layer transformation. If for the coarse-grain 
case the outermost loop is parallelized after appropriate loop 
coalescing, the fine-grain approach focusses on the innermost loops.
Similarly, this approach coalesces as much as possible inner loops 
so that enough work is generated for the fine-grain threads in 
the GPU device. It is this process what requires considerable programming 
efforts. The inner loop coalescing has to be done having in mind the 
work distribution mechanisms for GPU threads. Here we refer to the 
thread \emph{blocks} and \emph{grids}. In general, the more loops 
are coalesced, the more effective the parallelization. Regarding the 
programming efforts, after the loop transformation, the resulting loop 
has to be extracted and generate a GPU kernel. Also, introduce all 
necessary data transfers between host and guest device before and after 
the kernel execution. Clearly, when compared to the coarse-grain 
parallelization, the programming efforts are much more significant in 
the case of the fine-grain parallelization. At this point, what we 
have identified as network-agnostic feature becomes evident and 
important. Because the coarse-grain transformations are done from 
the outermost loop (batch-level parallelism) to the inner loops, 
the recoding efforts are independent from the layer transformation. 
The programmer has to parallelize the batch-level loop without any 
other consideration than apply the necessary data privatizations. 
That loop level is always parallel in the gradient descent algorithm. 
Then, if possible, coalesce with immediate inner loops for 
optimization. In the fine-grain parallelization, the programmer has to 
have knowledge of what is the nature of the layer computation. Also, 
has to know about the exact and optimal data layout to be generated. 
And finally, recode the implementation. For Caffe, this translates to
have a double implementation for CPU and GPU.

%\todo{coalescnng happens different. In gPU it happens fro the innermost loops. In CPU fro the outermost loops. GPU exploits finer grain,CPU coarser grain }
%\todo{emphasis in recoding, CPU version does not need recoding. }

%\todo{Caffe is already optimized to exploit the BLAS level parallelism with GPUs. 
%For some layer types, the Blob level parallelism is
%also exploited through the usage of GPU streams [REF]. The
%batch level of parallelism is not currently supported and corresponds 
%the target of this paper. Section XX describes the parallelization 
%process for the exploitation of this level of parallelism.}

%\begin{algorithm}
%%\small
%\caption{Fine-grain parallel layer forward phase}
%\label{alg-fine-par-forw}
%%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%\Input{Bottom blob (S, N+1, $D_1, D_2$, $\dots$, $D_N$)}
%\Output{Top blob for the layer transformation}
%\BlankLine
%\For{$s \leftarrow 1$ \KwTo $S$}{
%\BlankLine
%\For{$d_1 \leftarrow 1$ \KwTo $D_1$}{
    %\dots
%\BlankLine
    %\For{$d_k \leftarrow 1$ \KwTo $D_k$}{
      %\For{$civ \leftarrow 1$ \KwTo $D_{\text{k+1}}$*$\dots$*$D_N$}{
        %\BlankLine
        %$d_{\text{k+1}}$ = $f_{\text{k+1}}$(civ);
        %\BlankLine
        %\dots
        %\BlankLine
        %$d_N$ = $f_N$(civ);
        %\BlankLine
          %top[f(s, $d_1,d_2$, $\dots$, $d_N$)]=\textbf{BLAS}($W$($d_1,d_2$, $\dots$, $d_N$), bias($d_1,d_2$, $\dots$, $d_N$), bottom[g(s, $d_1,d_2$, $\dots$, $d_N$)]);
%}
%}
%}
%}
%\end{algorithm}

